{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device('cuda' if cuda_available else 'cpu')\n",
    "\n",
    "pin_memory = True if cuda_available else False\n",
    "batch_size = 128\n",
    "latent_size = 1024\n",
    "lr = 0.0003\n",
    "\n",
    "\n",
    "#print(np.mean(train_dataset.data.numpy())/255)\n",
    "#print(np.std(train_dataset.data.numpy())/255)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.5, std=0.5)\n",
    "    ])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data/mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# train_dataset.data = train_dataset.data[:1000]\n",
    "\n",
    "\n",
    "train = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=2, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.seq(x)\n",
    "        x = x[:, :, 0:28, 0:28]\n",
    "        return x\n",
    "        \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(512, 1,  kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "        \n",
    "G = Generator().to(device)\n",
    "\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='leaky_relu')\n",
    "    elif classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='leaky_relu')\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "G = G.apply(weights_init)\n",
    "D = D.apply(weights_init)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "G_opt = optim.AdamW(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "D_opt = optim.AdamW(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "def train_epoch(G_opt, D_opt, G, D, train):\n",
    "    \n",
    "    real_label = torch.tensor([1.0], device=device)\n",
    "    fake_label = torch.tensor([0.0], device=device)\n",
    "    \n",
    "    G.train()\n",
    "    D.train()\n",
    "    \n",
    "    dl = []\n",
    "    gl = []\n",
    "    \n",
    "    for i, (x, _) in enumerate(tqdm.tqdm(train,0)):\n",
    "        \n",
    "        x_real = x.to(device)\n",
    "        \n",
    "        D_opt.zero_grad(set_to_none=True)\n",
    "        \n",
    "        D_out_real = D(x_real).view(-1)\n",
    "        \n",
    "        y_real = real_label.repeat(D_out_real.shape[0],)\n",
    "        y_fake = fake_label.repeat(D_out_real.shape[0],)\n",
    "        \n",
    "        latent = torch.randn(D_out_real.shape[0], latent_size, 1, 1, device=device)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x_fake = G(latent)\n",
    "        \n",
    "        D_out_fake = D(x_fake).view(-1)\n",
    "        \n",
    "        D_real_loss = loss(D_out_real, y_real)\n",
    "        D_fake_loss = loss(D_out_fake, y_fake)\n",
    "        \n",
    "        D_loss = D_real_loss + D_fake_loss\n",
    "        D_loss.backward()\n",
    "        D_opt.step()\n",
    "        \n",
    "        G_opt.zero_grad(set_to_none=True)\n",
    "        \n",
    "        x_fake = G(latent)\n",
    "        D_out = D(x_fake).view(-1)\n",
    "        \n",
    "        G_loss = loss(D_out, y_real)\n",
    "        \n",
    "        G_loss.backward()\n",
    "        G_opt.step()\n",
    "        \n",
    "        dl.append(D_loss.item())\n",
    "        gl.append(G_loss.item())\n",
    "        \n",
    "    return np.mean(dl), np.mean(gl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain = True\n",
    "retrain = False\n",
    "epochs = 100\n",
    "\n",
    "if retrain:\n",
    "    for epoch in range(epochs):\n",
    "        dl, gl = train_epoch(G_opt, D_opt, G, D, train)\n",
    "        print(f\"Epoch: {epoch}, D_loss: {dl}, G_loss: {gl}\")\n",
    "        torch.save(G.state_dict(), f\"./gancheckpoints/generator_{epoch}.pth\")\n",
    "        torch.save(D.state_dict(), f\"./gancheckpoints/discriminator_{epoch}.pth\")\n",
    "        with torch.no_grad():\n",
    "            test_z = Variable(torch.randn(batch_size, 1024, 1, 1).to(device))\n",
    "            generated = G(test_z)\n",
    "            save_image(generated, './samples/sample_' + str(epoch) + '.png')\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    G.load_state_dict(torch.load(f\"./gancheckpoints/generator_{81}.pth\"))\n",
    "    D.load_state_dict(torch.load(f\"./gancheckpoints/discriminator_{81}.pth\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset with generated images and their latent space representations as the label\n",
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, G, num_samples):\n",
    "        self.G = G\n",
    "        self.num_samples = num_samples\n",
    "        self.z = torch.randn(num_samples, latent_size, 1, 1, device=device)\n",
    "        self.x = self.G(self.z)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.z[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1, 28, 28)\n",
      "(128, 1024, 1, 1)\n",
      "Train Epoch: 1 [0/1280 (0%)]\tLoss: 1.008980\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_50888/1830818088.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#batch_idx % 10 == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "#create a convnet to find the latent space representation of each image\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        #reshape the output to be the same shape as the latent space\n",
    "        output = x.view(-1, latent_size, 1, 1)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "model = Net().to(device)\n",
    "model.train()\n",
    "train_loader = torch.utils.data.DataLoader(GeneratedDataset(G, batch_size*10), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    with torch.no_grad():\n",
    "        print(data.cpu().numpy().shape)\n",
    "        print(target.cpu().numpy().shape)\n",
    "        break\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "for epoch in range(1, 3):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.mse_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if True:#batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timothe\\AppData\\Local\\Temp/ipykernel_50888/1876916207.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label1 = torch.nn.functional.one_hot(torch.tensor(label1), num_classes=10).float()\n",
      "C:\\Users\\Timothe\\AppData\\Local\\Temp/ipykernel_50888/1876916207.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label2 = torch.nn.functional.one_hot(torch.tensor(label2), num_classes=10).float()\n",
      "C:\\Users\\Timothe\\AppData\\Local\\Temp/ipykernel_50888/1876916207.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image1 = torch.tensor(image1).float().to(device).unsqueeze(0).unsqueeze(0)\n",
      "C:\\Users\\Timothe\\AppData\\Local\\Temp/ipykernel_50888/1876916207.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image2 = torch.tensor(image2).float().to(device).unsqueeze(0).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28)\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "#now we can generate new training data\n",
    "images = train_dataset.data\n",
    "labels = train_dataset.targets\n",
    "generated_images = []\n",
    "generated_labels = []\n",
    "for i in range(0,100):\n",
    "    #select 2 images at random\n",
    "    idx1 = np.random.randint(0,images.shape[0])\n",
    "    image1 = images[idx1]\n",
    "    label1 = labels[idx1]\n",
    "    label1 = torch.nn.functional.one_hot(torch.tensor(label1), num_classes=10).float()\n",
    "    idx2 = np.random.randint(0,images.shape[0])\n",
    "    image2 = images[idx2]\n",
    "    label2 = labels[idx2]\n",
    "    label2 = torch.nn.functional.one_hot(torch.tensor(label2), num_classes=10).float()\n",
    "\n",
    "    #find the latent space representation of each image\n",
    "    with torch.no_grad():\n",
    "        image1 = torch.tensor(image1).float().to(device).unsqueeze(0).unsqueeze(0)\n",
    "        image2 = torch.tensor(image2).float().to(device).unsqueeze(0).unsqueeze(0)\n",
    "        z1 = model(image1)\n",
    "        z2 = model(image2)\n",
    "\n",
    "    #interpolate between the two latent space representations\n",
    "    z = torch.lerp(z1, z2, 0.99)\n",
    "\n",
    "    #generate the label by combining the labels of the two images\n",
    "    label = torch.lerp(label1, label2, 0.99)\n",
    "\n",
    "    #generate the image\n",
    "    with torch.no_grad():\n",
    "        image = G(z)\n",
    "        image = image.cpu().numpy().squeeze()\n",
    "        label = label.cpu().numpy()\n",
    "        generated_images.append(image)\n",
    "        generated_labels.append(label)\n",
    "\n",
    "    #plot the generated image and the two original images\n",
    "    # fig, ax = plt.subplots(1,3)\n",
    "    # ax[0].imshow(image1.cpu().numpy().squeeze())\n",
    "    # ax[1].imshow(image2.cpu().numpy().squeeze())\n",
    "    # ax[2].imshow(image)\n",
    "    # print(label)\n",
    "\n",
    "    \n",
    "generated_images = np.array(generated_images)\n",
    "generated_labels = np.array(generated_labels)\n",
    "print(generated_images.shape)\n",
    "print(generated_labels.shape)\n",
    "#save the generated images and labels\n",
    "np.save(\"./datasets/generated_images.npy\", generated_images)\n",
    "np.save(\"./datasets/generated_labels.npy\", generated_labels)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
